{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early depression detection - data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_data_train = pd.read_csv(\"positive_examples_train.csv\")\n",
    "negative_data_train = pd.read_csv(\"negative_examples_train.csv\")\n",
    "positive_data = pd.read_csv(\"positive_examples.csv\") \n",
    "negative_data = pd.read_csv(\"negative_examples.csv\")\n",
    "\n",
    "positive_data['depressed'] = 1\n",
    "negative_data['depressed'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>depressed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>test_subject1345</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-05-20 05:12:23</td>\n",
       "      <td>So many unwanted Smith fadeaways.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>test_subject1345</td>\n",
       "      <td>Mid-Range Jumpers</td>\n",
       "      <td>2015-05-20 04:56:18</td>\n",
       "      <td>Hey guys, Celtics fan here pulling hard for th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>test_subject1345</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-05-01 05:51:20</td>\n",
       "      <td>Well he got 2 tonight so maybe he'll be able t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>test_subject1345</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-04-30 13:49:51</td>\n",
       "      <td>I mean he'll get pinch hits and an occasional ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>test_subject1345</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-04-30 05:47:34</td>\n",
       "      <td>Yeah you're probably right. Oh well.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18724</td>\n",
       "      <td>test_subject9942</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-09 13:36:09</td>\n",
       "      <td>it is compelling, because suicide is a popular...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18725</td>\n",
       "      <td>test_subject9942</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-09 13:31:32</td>\n",
       "      <td>you say you were raised Christian, you need to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18726</td>\n",
       "      <td>test_subject9942</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-09 13:26:23</td>\n",
       "      <td>I honestly couldn't think of title and I just ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18727</td>\n",
       "      <td>test_subject9942</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-09 13:20:55</td>\n",
       "      <td>not referring to those genres\\n\\nI mean what I...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18728</td>\n",
       "      <td>test_subject9942</td>\n",
       "      <td>Christianity is not a religion (that is it has...</td>\n",
       "      <td>2015-02-09 13:13:11</td>\n",
       "      <td>there are only 2 commandments as rules (there ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18729 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                              title  \\\n",
       "0      test_subject1345                                                NaN   \n",
       "1      test_subject1345                                  Mid-Range Jumpers   \n",
       "2      test_subject1345                                                NaN   \n",
       "3      test_subject1345                                                NaN   \n",
       "4      test_subject1345                                                NaN   \n",
       "...                 ...                                                ...   \n",
       "18724  test_subject9942                                                NaN   \n",
       "18725  test_subject9942                                                NaN   \n",
       "18726  test_subject9942                                                NaN   \n",
       "18727  test_subject9942                                                NaN   \n",
       "18728  test_subject9942  Christianity is not a religion (that is it has...   \n",
       "\n",
       "                      date                                               text  \\\n",
       "0      2015-05-20 05:12:23                  So many unwanted Smith fadeaways.   \n",
       "1      2015-05-20 04:56:18  Hey guys, Celtics fan here pulling hard for th...   \n",
       "2      2015-05-01 05:51:20  Well he got 2 tonight so maybe he'll be able t...   \n",
       "3      2015-04-30 13:49:51  I mean he'll get pinch hits and an occasional ...   \n",
       "4      2015-04-30 05:47:34               Yeah you're probably right. Oh well.   \n",
       "...                    ...                                                ...   \n",
       "18724  2015-02-09 13:36:09  it is compelling, because suicide is a popular...   \n",
       "18725  2015-02-09 13:31:32  you say you were raised Christian, you need to...   \n",
       "18726  2015-02-09 13:26:23  I honestly couldn't think of title and I just ...   \n",
       "18727  2015-02-09 13:20:55  not referring to those genres\\n\\nI mean what I...   \n",
       "18728  2015-02-09 13:13:11  there are only 2 commandments as rules (there ...   \n",
       "\n",
       "       depressed  \n",
       "0              1  \n",
       "1              1  \n",
       "2              1  \n",
       "3              1  \n",
       "4              1  \n",
       "...          ...  \n",
       "18724          1  \n",
       "18725          1  \n",
       "18726          1  \n",
       "18727          1  \n",
       "18728          1  \n",
       "\n",
       "[18729 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_unique = positive_data_train['id'].drop_duplicates()\n",
    "n_pos = len(pos_unique)\n",
    "n_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "403"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_unique = negative_data_train['id'].drop_duplicates()\n",
    "n_neg = len(neg_unique)\n",
    "n_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The overall error would be the\n",
    "# mean of the p ERDE values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERDE function\n",
    "def evaluation_metric(y_pred,y_true):\n",
    "    error = 0\n",
    "    cfp, cfn, ctp, ctn = (n_pos)/(n_pos+n_neg),1, 1, 0\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i]==1 and 0==y_true[i]:\n",
    "            error+= cfp\n",
    "        elif y_pred[i]==1 and 1==y_true[i]:\n",
    "            error+= method(i)\n",
    "        elif y_pred[i]==0 and 0==y_true[i]:\n",
    "            continue\n",
    "        elif y_pred[i]==0 and 1==y_true[i]:\n",
    "            error+= cfn\n",
    "        \n",
    "def method(i):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#cleaning unimportant stuff like links, etc..\n",
    "positive_data_train['text'] = positive_data_train['text'].astype(\"str\").map(str.strip)\n",
    "positive_data_train['text'] = positive_data_train['text'].apply(lambda x: re.sub('  ', ' ', x.lower()))\n",
    "positive_data_train['text'] = positive_data_train['text'].apply(lambda x: re.sub(r'http\\S+', '', x)) \n",
    "positive_data_train['text'] = positive_data_train['text'].apply(lambda x: re.sub(r\"www\\S+\", \"\", x)) \n",
    "positive_data_train['text'] = positive_data_train['text'].apply(lambda x: re.sub(r\"@\\S+\", \"\", x)) \n",
    "\n",
    "negative_data_train['text'] = negative_data_train['text'].astype(\"str\").map(str.strip)\n",
    "negative_data_train['text'] = negative_data_train['text'].apply(lambda x: re.sub('  ', ' ', x.lower()))\n",
    "negative_data_train['text'] = negative_data_train['text'].apply(lambda x: re.sub(r'http\\S+', '', x)) \n",
    "negative_data_train['text'] = negative_data_train['text'].apply(lambda x: re.sub(r\"www\\S+\", \"\", x)) \n",
    "negative_data_train['text'] = negative_data_train['text'].apply(lambda x: re.sub(r\"@\\S+\", \"\", x))\n",
    "\n",
    "#concating test data\n",
    "data_test = pd.concat([positive_data,negative_data])\n",
    "data_test['text'] = data_test['text'].astype(\"str\").map(str.strip)\n",
    "data_test['text'] = data_test['text'].apply(lambda x: re.sub('  ', ' ', x.lower()))\n",
    "data_test['text'] = data_test['text'].apply(lambda x: re.sub(r'http\\S+', '', x)) \n",
    "data_test['text'] = data_test['text'].apply(lambda x: re.sub(r\"www\\S+\", \"\", x)) \n",
    "data_test['text'] = data_test['text'].apply(lambda x: re.sub(r\"@\\S+\", \"\", x))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lematizing\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "positive_data_train['text_lem'] = [' '.join([WordNetLemmatizer().lemmatize(re.sub('[^A-Za-z]', '', line)) for line in lists.split()]).strip() for lists in positive_data_train['text']]\n",
    "negative_data_train['text_lem'] = [' '.join([WordNetLemmatizer().lemmatize(re.sub('[^A-Za-z]', '', line)) for line in lists]).strip() for lists in negative_data_train['text']] \n",
    "data_test['text_lem'] = [' '.join([WordNetLemmatizer().lemmatize(re.sub('[^A-Za-z]', '', line)) for line in lists.split()]).strip() for lists in data_test['text']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat data \n",
    "positive_data_train['depressed'] = 1\n",
    "negative_data_train['depressed'] = 0\n",
    "data_train = pd.concat([positive_data_train,negative_data_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.3, max_features=100, min_df=10, stop_words='english',ngram_range=(2,3), use_idf=True)\n",
    "X = vectorizer.fit_transform(positive_data_train['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "['10 minutes', '20 minutes', '30 minutes', 'best friend', 'big deal', 'couple years', 'depression anxiety', 'didn know', 'didn think', 'didn want', 'doesn make', 'doesn mean', 'doesn want', 'doesn work', 'don believe', 'don care', 'don feel', 'don know', 'don like', 'don mind', 'don need', 'don really', 'don remember', 'don think', 'don understand', 'don want', 'don worry', 'feel bad', 'feel better', 'feel free', 'feel like', 'feel way', 'feels like', 'felt like', 'good luck', 'good thing', 'hard time', 'hey guys', 'high school', 'just don', 'just feel', 'just got', 'just like', 'just need', 'just started', 'just want', 'just wanted', 'know just', 'know people', 'let know', 'like don', 'like just', 'like people', 'like said', 'little bit', 'll just', 'll try', 'long term', 'long time', 'look like', 'looks like', 'lot people', 'lot time', 'make feel', 'make sense', 'make sure', 'makes feel', 'makes sense', 'mental health', 'months ago', 'oh god', 'people don', 'people just', 'people like', 'people say', 'people think', 'pretty good', 'pretty sure', 'real life', 'really bad', 'really don', 'really good', 'really like', 'really want', 'sound like', 'sounds like', 'things like', 'think just', 'think people', 'tl dr', 've got', 've heard', 've read', 've seen', 've tried', 'want know', 'weeks ago', 'year old', 'years ago', 'years old']\n"
     ]
    }
   ],
   "source": [
    "print (X.toarray())\n",
    "print (vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting most important features from each model\n",
    "def show_most_informative_features(vect, clf, n=20):\n",
    "    feature_names = vect.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print(\"\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_grouped = data_train.groupby(['id'], as_index=False).agg(lambda x: x.tolist())\n",
    "data_train_grouped['text_lem'] = [' '.join(line) for line in lists]).strip() for lists in data_train_grouped['text']] \n",
    "data_test_grouped = data_test.groupby(['id'], as_index=False).agg(lambda x: x.tolist())\n",
    "data_test_grouped['text_lem'] = [' '.join(lists).strip() for lists in data_test_grouped['text']] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_grouped.loc[data_train_grouped.depressed.str[0]==1,'depressed']='1'\n",
    "data_train_grouped.loc[data_train_grouped.depressed.str[0]==0,'depressed']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_grouped.loc[data_test_grouped.depressed.str[0]==1,'depressed']='1'\n",
    "data_test_grouped.loc[data_test_grouped.depressed.str[0]==0,'depressed']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = text_clf_lr.predict(data_test_grouped.text_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "# nltk.download('vader_lexicon')\n",
    "# Sentiment score to be added to each users column\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "positive_data_train['sent']= positive_data_train.text_lem.apply(lambda x:sid.polarity_scores(x)['compound'])\n",
    "data_train['sentiment_compound_polarity']=data_train.text_lem.apply(lambda x:sid.polarity_scores(x)['compound'])\n",
    "data_train['sentiment_neutral']=data_train.text_lem.apply(lambda x:sid.polarity_scores(x)['neu'])\n",
    "data_train['sentiment_negative']=data_train.text_lem.apply(lambda x:sid.polarity_scores(x)['neg'])\n",
    "data_train['sentiment_pos']=data_train.text_lem.apply(lambda x:sid.polarity_scores(x)['pos'])\n",
    "data_train['sentiment_type']=''\n",
    "data_train.loc[data_train.sentiment_compound_polarity>0,'sentiment_type']='POSITIVE'\n",
    "data_train.loc[data_train.sentiment_compound_polarity==0,'sentiment_type']='NEUTRAL'\n",
    "data_train.loc[data_train.sentiment_compound_polarity<0,'sentiment_type']='NEGATIVE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_sentiment_depressed = data_train.loc[:,['depressed','sentiment_neutral', 'sentiment_type', 'text', 'text_lem']]\n",
    "data_train_sentiment_depressed = data_train_sentiment_depressed[data_train_sentiment_depressed.depressed == 1]\n",
    "\n",
    "# Plotting \n",
    "data_train_sentiment = data_train_sentiment_depressed.groupby(['sentiment_type'])['sentiment_neutral'].count()\n",
    "data_train_sentiment.rename(\"\",inplace=True)\n",
    "explode = (1, 0, 0)\n",
    "plt.subplot(221)\n",
    "data_train_sentiment.transpose().plot(kind='barh',figsize=(20, 20))\n",
    "plt.title('Sentiment of Tweets by Depressed People', bbox={'facecolor':'0.8', 'pad':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_sentiment_not_depressed = data_train.loc[:,['depressed','sentiment_neutral', 'sentiment_type', 'text', 'text_lem']]\n",
    "data_train_sentiment_not_depressed = data_train_sentiment_not_depressed[data_train_sentiment_not_depressed.depressed == 0]\n",
    "data_train_sentiment = data_train_sentiment_not_depressed.groupby(['sentiment_type'])['sentiment_neutral'].count()\n",
    "data_train_sentiment.rename(\"\",inplace=True)\n",
    "explode = (1, 0, 0)\n",
    "plt.subplot(221)\n",
    "data_train_sentiment.transpose().plot(kind='barh',figsize=(20, 20))\n",
    "plt.title('Sentiment of Tweets by Not Depressed People', bbox={'facecolor':'0.8', 'pad':0})\n",
    "\n",
    "data_train_sentiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plotting % of negative tweets \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# set width of bar\n",
    "barWidth = 0.25\n",
    " \n",
    "# set height of bar\n",
    "print (len(data_train_sentiment_depressed)/n_pos)\n",
    "print (len(data_train_sentiment_not_depressed)/n_neg)\n",
    "bars1 = [len(data_train_sentiment_depressed[data_train_sentiment_depressed.sentiment_type == \"NEGATIVE\"])/n_pos,len(data_train_sentiment_not_depressed[data_train_sentiment_not_depressed.sentiment_type == \"NEGATIVE\"])/n_neg]\n",
    "bars2 = [len(data_train_sentiment_depressed)/n_pos,len(data_train_sentiment_not_depressed)/n_neg]\n",
    " \n",
    "plt.subplot(2,1,1)\n",
    "\n",
    "# Set position of bar on X axis\n",
    "r1 = np.arange(len(bars1))\n",
    "r2 = [x + barWidth for x in r1]\n",
    " \n",
    "# Make the plot\n",
    "plt.bar(r1, bars1, color='#7f6d5f', width=barWidth, edgecolor='white', label='NEGATIVE SENTIMENT')\n",
    "plt.bar(r2, bars2, color='#557f2d', width=barWidth, edgecolor='white', label='ALL TWEETS')\n",
    " \n",
    "# Add xticks on the middle of the group bars\n",
    "plt.xlabel('GROUP', fontweight='bold')\n",
    "plt.xticks([r + barWidth for r in range(len(bars1))], ['DEPRESSED', 'NOT DEPRESSED'])\n",
    " \n",
    "# Create legend & Show graphic\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CALC % of negative tweets\n",
    "x = np.arange(2)\n",
    "\n",
    "data = [len(data_train_sentiment_depressed[data_train_sentiment_depressed.sentiment_type == \"NEGATIVE\"])/len(data_train_sentiment_depressed),\n",
    "        len(data_train_sentiment_not_depressed[data_train_sentiment_not_depressed.sentiment_type == \"NEGATIVE\"])/len(data_train_sentiment_not_depressed)]\n",
    "\n",
    "# Make the plot\n",
    "plt.bar(x, data,label='Percentage of negative posts')\n",
    "barWidth = 0.01\n",
    "# Add xticks on the middle of the group bars\n",
    "plt.xlabel('GROUP', fontweight='bold')\n",
    "plt.xticks([r + barWidth for r in range(len(data))], ['DEPRESSED', 'NOT DEPRESSED'])\n",
    " \n",
    "# Create legend & Show graphic\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(2)\n",
    "data = [len(data_train_sentiment_depressed)/n_pos,len(data_train_sentiment_not_depressed)/n_neg]\n",
    "\n",
    "# Make the plot\n",
    "plt.bar(x, data, label='NUMBER OF POSTS PER USER')\n",
    "barWidth = 0.01\n",
    "# Add xticks on the middle of the group bars\n",
    "plt.xlabel('GROUP', fontweight='bold')\n",
    "plt.xticks([r+barWidth for r in range(len(data))], ['DEPRESSED', 'NOT DEPRESSED'])\n",
    " \n",
    "# Create legend & Show graphic\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding I's in text\n",
    "data_train['text_len_word'] = data_train['text_lem'].apply(lambda x: re.findall(r'(\\S+)', x))\n",
    "data_train['text_len_word'] = data_train['text_len_word'].apply(len)\n",
    "data_train['text_i'] = data_train['text_lem'].apply(lambda x: re.findall(r'(?:\\s|^)i(?=\\s|$)', x))\n",
    "data_train['text_i'] = data_train['text_i'].apply(len)\n",
    "# Finding what percentage of \"i\" is of all posts words\n",
    "data_train['text_i_percent'] = (data_train['text_i']/data_train['text_len_word'])*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_train\n",
    "#Examining the difference between depressed and non-depressed \"i\" usage:\n",
    "per_of_i = data_train.groupby(['depressed'])['text_i_percent'].mean()\n",
    "per_of_i\n",
    "#Graphing it:\n",
    "x = np.arange(2)\n",
    "\n",
    "data = [per_of_i[1], per_of_i[0]]\n",
    "\n",
    "# Make the plot\n",
    "plt.bar(x, data,label='Percentage of I usage in posts')\n",
    "barWidth = 0.01\n",
    "# Add xticks on the middle of the group bars\n",
    "plt.xlabel('GROUP', fontweight='bold')\n",
    "plt.xticks([r + barWidth for r in range(len(data))], ['DEPRESSED', 'NOT DEPRESSED'])\n",
    " \n",
    "# Create legend & Show graphic\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_words = [\"absolutely\",\"all\",\"always\",\"complete\",\"completely\",\"constant\",\"constantly\",\"definitely\",\"entire\",\"ever\",\"every\",\"everyone\",\"everything\",\n",
    "             \"full\",\"must\",\"never\",\"nothing\",\"totally\",\"whole\"]\n",
    "\n",
    "def count_absolutist(x):\n",
    "    sum = 0\n",
    "    for word in abs_words:\n",
    "        sum+=x.count(word)\n",
    "    \n",
    "    return sum\n",
    "\n",
    "data_train_grouped['abs'] = data_train_grouped['text_lem'].apply(lambda x: count_absolutist(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_grouped['n_words'] = data_train_grouped['text_lem'].apply(lambda x: len(re.findall(r'\\w+', x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating percentage of absolutist words\n",
    "data_train_grouped['abs_p'] = (data_train_grouped['abs']/data_train_grouped['n_words'])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphing it\n",
    "per_of_abs = data_train_grouped.groupby(['depressed'])['abs_p'].mean()\n",
    "per_of_abs\n",
    "x = np.arange(2)\n",
    "\n",
    "data = [per_of_abs[1], per_of_abs[0]]\n",
    "\n",
    "# Make the plot\n",
    "plt.bar(x, data,label='Usage of absolutist words in posts by percentage')\n",
    "barWidth = 0.01\n",
    "# Add xticks on the middle of the group bars\n",
    "plt.xlabel('GROUP', fontweight='bold')\n",
    "plt.xticks([r + barWidth for r in range(len(data))], ['DEPRESSED', 'NOT DEPRESSED'])\n",
    " \n",
    "# Create legend & Show graphic\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_grouped['w_per_post'] = data_train_grouped['nwords']/data_train_grouped['pos_len']\n",
    "w_per_post = data_train_grouped.groupby(['depressed'])['w_per_post'].mean()\n",
    "w_per_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(2)\n",
    "data = [w_per_post[1], w_per_post[0]]\n",
    "\n",
    "# Make the plot\n",
    "plt.bar(x, data,label='Average words per post')\n",
    "barWidth = 0.01\n",
    "# Add xticks on the middle of the group bars\n",
    "plt.xlabel('GROUP', fontweight='bold')\n",
    "plt.xticks([r + barWidth for r in range(len(data))], ['DEPRESSED', 'NOT DEPRESSED'])\n",
    " \n",
    "# Create legend & Show graphic\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
